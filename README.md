<div align="center">
<h1>
    ğŸª‘ SeatWatch: Advanced Library Seat Occupancy Detection
</h1>

<p>
<strong>Real-time library seat occupancy detection using YOLOv7 object detection and tracking</strong>
</p>

<div>
    <img width="500" alt="teaser" src="doc/logo.webp">
</div>

<div>
  <a target="_blank" href="https://colab.research.google.com/github/asumansaree/Library-Seat-Occupancy-Detection">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
  </a>
  <a href="#installation">
    <img src="https://img.shields.io/badge/python-3.8+-blue.svg" alt="Python 3.8+"/>
  </a>
  <a href="#license">
    <img src="https://img.shields.io/badge/license-MIT-green.svg" alt="License"/>
  </a>
</div>
</div>

---

## ğŸ¯ About The Project

SeatWatch is an AI-powered system that monitors library seat occupancy in real-time using computer vision. It detects when seats are occupied (with or without a person present) and tracks how long belongings have been left unattended, helping libraries manage space more efficiently.

### ğŸŒŸ Key Features

- **Real-time Detection**: Uses YOLOv7 for accurate person and chair detection
- **Object Tracking**: Advanced SORT algorithm tracks individual seats over time  
- **Occupancy Timing**: Monitors how long seats remain occupied without a person
- **Automated Alerts**: Flags seats held too long with "TIME EXCEEDED" warnings
- **Video Processing**: Supports various video formats (MP4, AVI, etc.)

<img src="doc/project-description.png" alt="Project Overview" width="500"/>

### ğŸ—ï¸ System Architecture

<img src="doc/system-architecture.png" alt="System Architecture" width="500" />

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+** installed on your system
- **~3GB** free disk space (for dependencies and models)
- **Internet connection** for downloading models

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/priyanshuharshbodhi1/Library-Seat-Occupancy-Detection
   cd Library-Seat-Occupancy-Detection
   ```

2. **Set up virtual environment** (recommended)
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Download model weights**
   ```bash
   python download_models.py
   ```
   This downloads the YOLOv7 weights (~74MB) required for detection.

5. **Verify installation**
   ```bash
   python -c "import cv2, torch, numpy; print('âœ… Installation successful!')"
   ```

---

## ğŸ’» Usage

### Option 1: REST API (Recommended for Production)

The project now includes a full-featured REST API for video processing via HTTP requests.

```bash
# Quick start with Python script
python run_api.py

# Or with Docker
docker-compose up -d
```

**Upload video and get results:**
```python
import requests

# Upload video
response = requests.post(
    "http://localhost:8000/api/detect",
    files={"video": open("library_video.mp4", "rb")}
)
job_id = response.json()["job_id"]

# Check status
status = requests.get(f"http://localhost:8000/api/jobs/{job_id}").json()

# Download results
requests.get(f"http://localhost:8000/api/download/{job_id}")
```

ğŸ“– **Full API Documentation**: See [API_README.md](API_README.md)

**API Features:**
- ğŸŒ RESTful HTTP endpoints
- ğŸ“¤ Video upload via multipart/form-data
- ğŸ“Š JSON results with detection statistics
- ğŸ¥ Download processed videos
- ğŸ“ˆ Real-time progress tracking
- ğŸ³ Docker support
- ğŸ“ Interactive API docs at `/docs`

---

### Option 2: Command Line (Direct Processing)

#### Basic Usage
```bash
python detect_and_track.py --source your_video.mp4
```

#### Advanced Usage
```bash
python detect_and_track.py \
    --weights yolov7.pt \
    --source "library_video.mp4" \
    --conf-thres 0.4 \
    --classes 0 56 \
    --name "Library_Detection_Run" \
    --view-img
```

#### Command Line Arguments

| Argument | Description | Default | Example |
|----------|-------------|---------|---------|
| `--source` | Input video file path | **Required** | `"video.mp4"` |
| `--weights` | Model weights file | `yolov7.pt` | `yolov7.pt` |
| `--conf-thres` | Detection confidence threshold | `0.25` | `0.4` |
| `--classes` | Specific classes to detect | All classes | `0 56` |
| `--name` | Output experiment name | Auto-generated | `"my_test"` |
| `--view-img` | Show real-time video preview | `False` | Add flag |

#### Detected Classes
- **Class 0**: Person (human detection) ğŸ‘¤
- **Class 56**: Chair (furniture detection) ğŸª‘  
- Use `--classes 0 56` to detect only people and chairs for better performance

---

## ğŸ“ Project Structure

```
Library-Seat-Occupancy-Detection/
â”œâ”€â”€ ğŸ“„ detect_and_track.py      # Main detection script
â”œâ”€â”€ ğŸ“„ sort.py                  # SORT tracking algorithm
â”œâ”€â”€ ğŸ“„ download_models.py       # Model download utility
â”œâ”€â”€ ğŸ“„ requirements.txt         # Python dependencies
â”œâ”€â”€ ğŸ“‚ models/                  # YOLOv7 model architecture
â”œâ”€â”€ ğŸ“‚ utils/                   # Helper functions and utilities
â”œâ”€â”€ ğŸ“‚ data/                    # Configuration files
â”œâ”€â”€ ğŸ“‚ doc/                     # Documentation and images
â”œâ”€â”€ ğŸ“‚ runs/                    # Output videos (auto-created)
â”œâ”€â”€ ğŸ§  yolov7.pt               # Model weights (downloaded)
â””â”€â”€ ğŸ“„ README.md               # This file
```

---

## ğŸ“Š Sample Output

### Output Location
Results are automatically saved to:
```
runs/detect/{experiment_name}/your_video.mp4
```

### Visual Features
- ğŸ”´ **Red boxes**: People with unique tracking IDs
- ğŸŸ¡ **Yellow boxes**: Chairs and furniture  
- â° **Timing info**: Duration of seat occupancy
- âš ï¸ **Alerts**: "TIME EXCEEDED" for seats held too long

<img src="doc/output.png" alt="Sample Detection Output" width="500" />

---

## ğŸ› ï¸ Troubleshooting

<details>
<summary><strong>ğŸ Python Environment Issues</strong></summary>

**ModuleNotFoundError: No module named 'cv2'**
```bash
# Ensure virtual environment is activated
source venv/bin/activate

# Reinstall OpenCV
pip install opencv-python
```

**Virtual environment not working**
```bash
# Recreate environment
rm -rf venv
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```
</details>

<details>
<summary><strong>ğŸ“¦ Model and File Issues</strong></summary>

**Model weights not found**
```bash
# Run the download script
python download_models.py

# Or download manually
wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt
```

**Video file issues**
```bash
# Check supported formats: MP4, AVI, MOV, MKV
# Ensure video file path is correct
ls -la your_video.mp4
```
</details>

<details>
<summary><strong>âš¡ Performance Issues</strong></summary>

**Slow processing**
- Use smaller videos for testing (< 1GB)
- Increase `--conf-thres` to 0.4 or higher
- Close other applications
- Consider using GPU if available

**High memory usage**
- Process shorter video clips
- Reduce video resolution
- Monitor with `htop` or Task Manager
</details>

---

## ğŸ”§ Development

### Running Tests
```bash
# Test with sample data
python detect_and_track.py --source data/sample_video.mp4 --view-img
```

### Contributing
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“– Technical Details

For detailed technical documentation and research background, see:
- [ğŸ“‹ Technical Documentation](https://docs.google.com/document/d/1pJ2VXuCuY54If5JJqTePYmlqCHkCWHwM6suYYAaN2k4/edit?usp=sharing)
- [ğŸ”¬ Research Papers](#references)

---

## ğŸ’¾ System Requirements

| Component | Requirement |
|-----------|-------------|
| **OS** | Windows 10+, macOS 10.14+, Ubuntu 18.04+ |
| **Python** | 3.8 - 3.11 |
| **RAM** | 8GB minimum, 16GB recommended |
| **Storage** | 3GB free space |
| **GPU** | Optional (CUDA-compatible for faster processing) |

---

## ğŸ“ Contact & Support

- **Author**: Asuman Sare ERGUT  
- **Email**: asumansaree@gmail.com
- **Issues**: [GitHub Issues](https://github.com/priyanshuharshbodhi1/Library-Seat-Occupancy-Detection/issues)

---

## ğŸ“œ References

- [YOLOv7 Object Tracking](https://github.com/RizwanMunawar/yolov7-object-tracking)
- Anish Aralikatti et al 2020 J. Phys.: Conf. Ser. 1706 012149  
- Redmon J, Divvala S, Girshick R and Farhadi A You Only Look Once: Unified, Real-Time Object Detection 2016 IEEE Conference on Computer Vision and Pattern Recognition
- [COCO 2017 Dataset](http://cocodataset.org)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">
<p>Made with â¤ï¸ for smarter library management</p>
</div>
